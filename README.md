# **Log Classification with Hybrid Classification Framework**

This project implements an advanced **hybrid log classification** framework that intelligently combines three complementary classification strategies. The framework is designed to handle **different levels of complexity** in log messages—ranging from simple, rule-based patterns to highly complex, low-data scenarios.

The goal is to provide a **robust, flexible, and scalable solution** for categorizing logs in real-world scenarios, where log data can vary in structure, consistency, and labeling quality.

## **1. Introduction**

Logs generated by systems, applications, or devices can vary greatly:

* Some follow **predictable formats**.
* Some contain **complex patterns** requiring machine learning.
* Others are **messy, unlabeled, or inconsistent**.

This project addresses all three cases by combining:

1. **Regex-based classification** for simple, recurring patterns.
2. **Sentence Transformer + Logistic Regression** for complex, labeled data.
3. **Large Language Model (LLM) reasoning** for edge cases with insufficient labeled data.

By using the **right tool for the right problem**, the framework achieves **higher accuracy and adaptability** than a single-method classifier.

---

## **2. Classification Approaches**

### **A. Regular Expression (Regex)**

* **Purpose**: Best suited for **simple, structured, and repetitive patterns** (e.g., error codes, timestamp formats, known keywords).
* **Advantages**:

  * Extremely fast and lightweight.
  * No training required.
* **Limitations**:

  * Not suitable for evolving or noisy log formats.
* **Example**:

 
---

### **B. Sentence Transformer + Logistic Regression**

* **Purpose**: Handles **complex patterns** where sufficient **labeled training data** is available.
* **How it works**:

  1. **Sentence Transformers** convert log messages into high-dimensional embeddings capturing semantic meaning.
  2. **Logistic Regression** classifies the embeddings into target categories.
* **Advantages**:

  * Learns from historical data.
  * Captures contextual relationships in logs.
* **Limitations**:

  * Requires enough labeled examples for good performance.


---

### **C. Large Language Model (LLM)**

* **Purpose**: Acts as a **fallback** when training data is insufficient for ML models.
* **How it works**:

  * Uses an API-based or locally hosted LLM  to infer labels based on prompt engineering.
  * Can also refine predictions made by regex or ML models.
* **Advantages**:

  * No need for labeled datasets.
  * Can adapt to unseen patterns.
* **Limitations**:

  * Higher computational cost.
  * May require prompt tuning for consistency.

---

## **3. System Architecture**

The architecture follows a **sequential fallback design**:

1. **Regex Layer** → Quickly labels logs that match known patterns.
2. **ML Layer** → Processes remaining logs with Sentence Transformer embeddings + Logistic Regression.
3. **LLM Layer** → Handles all remaining unclassified or ambiguous logs.


## **4. Advantages of This Hybrid Approach**

* **Speed** – regex handles trivial cases instantly.
* **Accuracy** – ML handles nuanced semantic patterns.
* **Flexibility** – LLM adapts to unseen or rare logs.
* **Scalability** – Easily extend with more patterns or models.

---
